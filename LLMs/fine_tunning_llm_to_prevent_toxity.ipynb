{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ed9152-9dca-49ec-b03b-7b7ca64edf5c",
   "metadata": {},
   "source": [
    "# Fine-tunning de uma LLM utilizando o aprendizado por reforço busncando prevenir respostas negativos\n",
    "\n",
    "Retornos negativos de modelos de LLM podem ocorrer quando este não é treinado de maneira eficiente para prevenir esse tipo de ocorrência. Uma das principais técnicas utilizadas atualmente para previnir retornos que podem conter caráter ofensivo (tanto em aspecto ético, como legal) é a utilização de modelos de aprendizado por reforço, que realizam o ajuste do modelo de LLM até que a sua reposta esteja de acordo com o esperado.\n",
    "\n",
    "Summário:\n",
    "\n",
    "- [1 - Configurando o Ambiente](#1)\n",
    "   - [1.1 - Verifica a Instancia e Instala os Utilitários que serão Necessários](#1.1)\n",
    "   - [1.2 - Importa as Bibliotecas que serão utilizadas](#1.2) \n",
    "- [2 - Carrega o modelo FLAN-T5, Prepara o Modelo de Recompensa e a Avaliação da Toxicidade](#2)\n",
    "   - [2.1 - Preparando o modelo de recompensa](#2.1)\n",
    "- [3 - Aplicando o PPO para Retirar a Toxidade do modelo](#3)\n",
    "   - [3.1 - Inicializa o PPOTrainer](#3.1)\n",
    "   - [3.2 - Realiza o Fine-tunning do modelo com a tecnica de POO](#3.2)\n",
    "   - [3.3 - Avalia o modelo qualitativamente](#3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a8b73-aa65-40cc-abd8-9f4068e831f8",
   "metadata": {},
   "source": [
    "<a name='1' ></a>\n",
    "# 1 - Configurando o Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93668e2f",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Verifica a Instancia e Instala os Utilitários que serão Necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a021d13e-7801-484c-8e66-135887ed258d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected instance type: instance-datascience-ml-m5-2xlarge\n",
      "Current instance type: ml-m5-2xlarge\n",
      "Instance type has been choosen correctly\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "instance_type_expected = \"ml-m5-2xlarge\"\n",
    "instance_type_current = os.environ.get(\"HOSTNAME\")\n",
    "\n",
    "print(f\"Expected instance type: instance-datascience-{instance_type_expected}\")\n",
    "print(f\"Current instance type: {instance_type_expected}\")\n",
    "\n",
    "assert  instance_type_expected in instance_type_current, f\"ERROR. Expected instance type: {instance_type_expected}\"\n",
    "print(f\"Instance type has been choosen correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e9ae06-2700-4145-8fb7-073f115f59f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.17.0\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets==2.17.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.64.1)\n",
      "Collecting xxhash (from datasets==2.17.0)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.15)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.17.0)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.17.0)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from datasets==2.17.0) (6.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.17.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.17.0)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.17.0)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.17.0)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.17.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
      "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, multidict, fsspec, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.17.0 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.21.3 multidict-6.0.5 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+http://github.com/lvwerra/trl.git@25fa1bd\n",
      "  Cloning http://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-nx1a_hks\n",
      "  Running command git clone --filter=blob:none --quiet http://github.com/lvwerra/trl.git /tmp/pip-req-build-nx1a_hks\n",
      "  warning: redirecting to https://github.com/lvwerra/trl.git/\n",
      "\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 25fa1bd\n",
      "  warning: redirecting to https://github.com/lvwerra/trl.git/\n",
      "  Resolved http://github.com/lvwerra/trl.git to commit 25fa1bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (4.27.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.26.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (0.27.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (2.17.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (69.0.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (0.42.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (0.4.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->trl==0.4.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.18.0->trl==0.4.2.dev0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n",
      "Building wheels for collected packages: trl\n",
      "  Building wheel for trl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67532 sha256=290cfb6b7206218c77174add4f6a8629d9446e234dfd320737d54d3019836901\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ltasuxnb/wheels/a4/61/4c/890f0c380cb01b9ff2e7ce5e25437b6266127cb50bf569052a\n",
      "Successfully built trl\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.4.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    peft==0.3.0 --quiet\n",
    "\n",
    "# Installing Reinforcement learning direct from library\n",
    "%pip install git+http://github.com/lvwerra/trl.git@25fa1bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c7cbdd-69bd-4781-8dd0-7ad0bcbc64af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "                            AutoModelForSeq2SeqLM, GenerationConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# tlr: transformer reinforcement learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library make the loops show a smart process meter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10f188-446c-432b-83d3-2d86bd2a9089",
   "metadata": {},
   "source": [
    "<a name=2></a>\n",
    "# 2 - Carrega o modelo FLAN-T5, Prepara o Modelo de Recompensa e a Avaliação da Toxicidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251b68f-dc39-4fbc-b7d9-8a77cdb0de86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3456b2d-79d1-4b21-bb61-b8c8a6cc9b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6305ba4067454698239968eedf0f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e2bea11c3a46a8b5123eb392051884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2a678209dc4a2da3879298931c4f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524a3c23be9a43949b5dbf50cbca82c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ab90f4576f4c4b9f8f8dec1e37ae17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73ea2273e14f6faa4465ec6559e8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b075428b40451caf60d47a2506ef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdde2eeed2464368987778eba3fe8192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae129eb31de34301bca3f74bad476387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97b6de193f94e4d99bff8ded928f248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf91191f9484277bc5a4db7f006663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb859a7e70b640a593be5c8a363ef4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "original_dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ce7c919-b763-48f6-8a66-db85ab2e3be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(dataset_name, model_name, input_min_length, input_max_length):\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Filter dialogues lens into min and max input length\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_length and len(x[\"dialogue\"]) <= input_max_length, batched=False)\n",
    "    \n",
    "    # Prepare the tokenizer. Set device_map='Auto' allows sweet from CPU to GPU automatically\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        \n",
    "        Summaryze the following conversation.\n",
    "        \n",
    "        {sample[\"dialogue\"]}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "        \n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # This must be called \"query\". Which is a requirement of PPO library\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    \n",
    "    # Tokenize the dataset dialogues\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Split data into train and test parts\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "    \n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(huggingface_dataset_name, model_name, input_min_length=200, input_max_length=1000) \n",
    "    \n",
    "print(dataset)              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db2037-0d5a-4c55-a078-019edbe42a5b",
   "metadata": {},
   "source": [
    "Download the fully peft pre-trained model from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "576b23b3-59a1-46ae-820b-4706b71245a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3d27a9f-2186-457e-8c1b-1097a538db5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 14M May 15  2023 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721e1f2-e6f4-4493-9695-01af8b60206a",
   "metadata": {},
   "source": [
    "No laboratio experimento anterio foi incluido um adaptador em que todos os pesos foram congelados, somente para inferencia.\n",
    "Agora, as configurações do Lora são necessários pois o adaptador será treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba782c42-b139-4e11-ab4e-b4d833352328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_model_trainable_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():   # Parametros de toda a rede\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params +=param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}. Model parameters:{all_model_params}. Percentage of trainable model parameters: {100*(trainable_model_params/all_model_params)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ead0f800-56a2-426a-ba43-0670f00bf825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model parameters to be update: trainable model parameters: 3538944. Model parameters:251116800. Percentage of trainable model parameters: 1.4092820552029972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,   # Rank,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM   # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       \"./peft-dialogue-summary-checkpoint-from-s3/\",\n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       device_map=\"auto\",\n",
    "                                       is_trainable=True\n",
    "                                      )\n",
    "\n",
    "print(f\"Peft model parameters to be update: {print_number_of_model_trainable_parameters(peft_model)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639f941-e9e4-440a-81a1-2bf47ad35375",
   "metadata": {},
   "source": [
    "Neste laboratorio esta sendo pre-treinado um LLM utilizando-se para isto a tecnica PEFT/Lora. E, para o treinamento utilizando o metodo Lora,\n",
    "um algoritmo de aprendizado por reforço é utilizado. O aprendizado por reforço utiliza uma tecnica de PPO para otimização da política RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1167be24-14ab-485c-9b13-09c1c797d48e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.336, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be update: trainable model parameters: 3539713. Model parameters:251117569. Percentage of trainable model parameters: 1.4095839706062143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                               torch_dtypte=torch.bfloat16,\n",
    "                                                               is_trainable=True\n",
    "                                                              ) \n",
    "print(f\"PPO model parameters to be update: {print_number_of_model_trainable_parameters(ppo_model)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45858e79-17d0-4f54-b613-099129142c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Cria uma copia do modelo sem o ajuste da toxidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bccd3035-9cc2-4ad4-81ce-5b6489c24f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_model = create_reference_model(ppo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272645d-8e76-4c8f-9e91-dce7db673a14",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.1'></a>\n",
    "## 2.1 - Preparando o modelo de recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "593ac042-ee82-452d-900c-fe33ec8ff19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb581a-ffc9-4898-8cd7-f1b28b65b716",
   "metadata": {},
   "source": [
    "Pega alguns exemplo e passa para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf636420-75a9-4a8c-a66a-1b4e63cf2e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of logit function: [not hate, hate]: [3.1276395320892334, -2.4689579010009766]\n",
      "probabilities for [not hate, hate]: [0.9963032007217407, 0.003696749685332179]\n",
      "output of logit function: [not hate]. This is the reward: 3.1276395320892334\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"Person 1# tells tony that he didn't like the movie\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors='pt').input_ids\n",
    "\n",
    "# Aplica a transormacao utilizando a funcao logistica\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f\"outputs of logit function: [not hate, hate]: {logits.tolist()[0]}\")\n",
    "      \n",
    "# Imprime as probabilidade para [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f\"probabilities for [not hate, hate]: {probabilities}\")\n",
    "      \n",
    "# Pega a saida da funcao logistica para [not hate]. Essa é a recompensa!\n",
    "not_hate_index = 0\n",
    "not_hate_logit = logits.tolist()[0][not_hate_index]\n",
    "print(f\"output of logit function: [not hate]. This is the reward: {not_hate_logit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9531dc-14ea-484f-8bfc-167064965cdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Vamos ver agora um exemplo de um comentario negativo. A \"recompensa\" vai ser menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc86d4c1-d9e8-4726-83b7-3322a308996b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of logit function: [not hate, hate]: [-0.6807236075401306, 0.3578655421733856]\n",
      "probabilities for [not hate, hate]: [0.2614223062992096, 0.738577663898468]\n",
      "output of logit function: [not hate]. This is the reward: -0.6807236075401306\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"Person 1# tells tony that this movie is terrible and stupid.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors='pt').input_ids\n",
    "\n",
    "# Aplica a transormacao utilizando a funcao logistica\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f\"outputs of logit function: [not hate, hate]: {logits.tolist()[0]}\")\n",
    "      \n",
    "# Imprime as probabilidade para [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f\"probabilities for [not hate, hate]: {probabilities}\")\n",
    "      \n",
    "# Pega a saida da funcao logistica para [not hate]. Essa é a recompensa!\n",
    "not_hate_index = 0\n",
    "not_hate_logit = logits.tolist()[0][not_hate_index]\n",
    "print(f\"output of logit function: [not hate]. This is the reward: {not_hate_logit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0518c-df53-421a-a26a-1ec6cd74d3d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Agora, utilizando a biblioteca hugging face para simplificar o codigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a449aee6-2b6f-4948-b80a-41b98434d291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output\n",
      "For non-toxicity text\n",
      "[{'label': 'nothate', 'score': 3.1276395320892334}, {'label': 'hate', 'score': -2.4689579010009766}]\n",
      "[{'label': 'nothate', 'score': 0.9963032007217407}, {'label': 'hate', 'score': 0.003696749685332179}]\n",
      "For toxicity text\n",
      "[{'label': 'hate', 'score': 0.3578655421733856}, {'label': 'nothate', 'score': -0.6807236075401306}]\n",
      "[{'label': 'hate', 'score': 0.738577663898468}, {'label': 'nothate', 'score': 0.2614223062992096}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                           model=toxicity_model_name,\n",
    "                           device=device)\n",
    "reward_logits_kwards = {\n",
    "    \"top_k\":None,   # Return all score \n",
    "    \"function_to_apply\":\"none\",   # Return the output of the logistic function only\n",
    "    \"batch_size\":16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwards = {\n",
    "    \"top_k\":None,   # Return all score \n",
    "    \"function_to_apply\":\"softmax\",   # Return the output of the logistic after apply the softmax funtion\n",
    "    \"batch_size\":16\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Reward model output\")\n",
    "print(\"For non-toxicity text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwards))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwards))\n",
    "print(\"For toxicity text\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwards))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ba12f-b621-4534-b06a-a0d6087686f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.3'></a>\n",
    "2.3- Avaliando a toxidade do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b36f17cc-dbf2-424d-800e-e46494d7e495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                  toxicity_model_name,\n",
    "                                  module_type=\"measurement\",\n",
    "                                  toxic_label=\"hate\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f650f-dcb5-4728-85c3-850dfe8fc79d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Calcula o score de toxidade. Não é surpresa que este seja igual a saída da função logistica para a classe [hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c82ba3aa-7f8b-44f1-ba8c-ab57a886c9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non toxic text toxicity measurement: {'toxicity': [0.003696749685332179]}\n",
      "Toxic text toxicity measurement: {'toxicity': [0.738577663898468]}\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[non_toxic_text])\n",
    "\n",
    "print(f\"Non toxic text toxicity measurement: {toxicity_score}\")\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[toxic_text])\n",
    "\n",
    "print(f\"Toxic text toxicity measurement: {toxicity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c0a30-8a58-4caa-ac2b-ec7759cb96a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Este modelo pode ser utilizado para avaliar o score dos dialogos carregados na seção [2.1](#1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12c6400f-f7d0-460e-9169-52de58fe2ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model, toxicity_evaluator, tokenizer, dataset, num_samples):\n",
    "    \n",
    "    \n",
    "    max_new_tokens = 100\n",
    "    \n",
    "    toxicities = []\n",
    "    \n",
    "    input_texts = []\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        imput_text = sample[\"query\"]\n",
    "        \n",
    "        if i > num_samples:\n",
    "            break\n",
    "        \n",
    "        input_ids = tokenizer(imput_text, return_tensors='pt', padding=True).input_ids\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             top_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_samples=True)\n",
    "        \n",
    "        response_tokens_ids = model.generate(input_ids=input_ids,\n",
    "                                             generation_config=generation_config)\n",
    "        \n",
    "        generation_text = tokenizer.decode(response_tokens_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(imput_text + \"\" + generation_text)])\n",
    "        \n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "        \n",
    "    # Compute mean and std\n",
    "    mean_toxicity = np.mean(toxicities)\n",
    "    std_toxicity = np.std(toxicities)\n",
    "    \n",
    "    return mean_toxicity, std_toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13aa54-1e9e-43a7-b70c-bc7a3dfc0a77",
   "metadata": {},
   "source": [
    "Verifica o desempenho do modelo anteriormente ao fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b260ed08-f7ce-48bd-bb5d-ec4ef4a124cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:22,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity mean and std before fine-tunning, mean: 0.01768248436168175 - std: 0.022171593557173273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_fine_tunning, std_before_fine_tunning = evaluate_toxicity(model=ref_model,\n",
    "                                                                      toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                      tokenizer=tokenizer,\n",
    "                                                                      dataset=dataset[\"test\"],\n",
    "                                                                      num_samples=10)\n",
    "\n",
    "print(f\"toxicity mean and std before fine-tunning, mean: {mean_before_fine_tunning} - std: {std_before_fine_tunning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37bad6-95f0-451a-ae91-c668354da985",
   "metadata": {},
   "source": [
    "<a name=\"3\" ></a>\n",
    "# 3 - Aplicando o PPO para Retirar a Toxidade do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d42709-2bbd-4e2c-b65b-efc9052c034d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"3.1\" ></a>\n",
    "## 3.1 - Inicializa o PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a85131c8-d549-45f7-901d-bc5dc0a595a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator of the test data: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
    "\n",
    "print(f\"Test data: {test_data}\")\n",
    "print(f\"Collator of the test data: {collator(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29ba47c8-7fa2-45d2-b0dc-61f7b409e0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.336, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.41e-5\n",
    "max_ppo_epochs = 1\n",
    "mini_batch_size = 4\n",
    "batch_size = 16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config,\n",
    "                         model=ppo_model,\n",
    "                         ref_model=ref_model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset[\"train\"],\n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a178949-6345-4f7b-9198-4f03045d28b9",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "## 3.2 - Realiza o Fine-tunning do modelo com a tecnica de POO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b677f-0b3d-4953-8da2-7867b5c55bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'nothate', 'score': 2.9155023097991943}, {'label': 'hate', 'score': -2.291459560394287}], [{'label': 'nothate', 'score': 2.6924073696136475}, {'label': 'hate', 'score': -2.123380661010742}], [{'label': 'nothate', 'score': 1.6082422733306885}, {'label': 'hate', 'score': -1.249406337738037}], [{'label': 'nothate', 'score': 2.9333224296569824}, {'label': 'hate', 'score': -2.4180214405059814}], [{'label': 'nothate', 'score': 2.5497634410858154}, {'label': 'hate', 'score': -2.025524139404297}], [{'label': 'nothate', 'score': 2.162712574005127}, {'label': 'hate', 'score': -1.695723056793213}], [{'label': 'nothate', 'score': 1.4694528579711914}, {'label': 'hate', 'score': -1.1428331136703491}], [{'label': 'nothate', 'score': 1.6313555240631104}, {'label': 'hate', 'score': -1.326237678527832}], [{'label': 'nothate', 'score': 1.577005386352539}, {'label': 'hate', 'score': -1.266847848892212}], [{'label': 'nothate', 'score': 3.3155808448791504}, {'label': 'hate', 'score': -2.674252510070801}], [{'label': 'nothate', 'score': 1.976147174835205}, {'label': 'hate', 'score': -1.480954885482788}], [{'label': 'nothate', 'score': 1.8680100440979004}, {'label': 'hate', 'score': -1.4659287929534912}], [{'label': 'nothate', 'score': 2.4998779296875}, {'label': 'hate', 'score': -1.9669795036315918}], [{'label': 'nothate', 'score': 2.1732475757598877}, {'label': 'hate', 'score': -1.6615629196166992}], [{'label': 'nothate', 'score': 1.948746681213379}, {'label': 'hate', 'score': -1.5295898914337158}], [{'label': 'nothate', 'score': 2.729102849960327}, {'label': 'hate', 'score': -2.1531479358673096}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:47, 107.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 37.28555679321289\n",
      "ppo/returns/mean: -1.0773183107376099\n",
      "ppo/policy/advantages_mean: -7.3118471277666686e-09\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 2.124241590499878}, {'label': 'hate', 'score': -1.6787097454071045}], [{'label': 'nothate', 'score': 1.8615583181381226}, {'label': 'hate', 'score': -1.4675705432891846}], [{'label': 'nothate', 'score': 1.8002383708953857}, {'label': 'hate', 'score': -1.3934768438339233}], [{'label': 'nothate', 'score': 3.0695385932922363}, {'label': 'hate', 'score': -2.4361748695373535}], [{'label': 'nothate', 'score': 2.0829241275787354}, {'label': 'hate', 'score': -1.6129119396209717}], [{'label': 'nothate', 'score': 2.078359603881836}, {'label': 'hate', 'score': -1.5973446369171143}], [{'label': 'nothate', 'score': 4.1084465980529785}, {'label': 'hate', 'score': -3.489579916000366}], [{'label': 'nothate', 'score': 2.1222023963928223}, {'label': 'hate', 'score': -1.6572320461273193}], [{'label': 'nothate', 'score': 2.146883487701416}, {'label': 'hate', 'score': -1.603076696395874}], [{'label': 'nothate', 'score': 1.3037939071655273}, {'label': 'hate', 'score': -1.031118631362915}], [{'label': 'nothate', 'score': 1.0476409196853638}, {'label': 'hate', 'score': -0.8274179697036743}], [{'label': 'nothate', 'score': 1.5700119733810425}, {'label': 'hate', 'score': -1.2701655626296997}], [{'label': 'nothate', 'score': 1.5085852146148682}, {'label': 'hate', 'score': -1.1708998680114746}], [{'label': 'nothate', 'score': 1.250786542892456}, {'label': 'hate', 'score': -0.9757324457168579}], [{'label': 'nothate', 'score': 2.872171640396118}, {'label': 'hate', 'score': -2.264605760574341}], [{'label': 'nothate', 'score': 3.841696262359619}, {'label': 'hate', 'score': -3.196878433227539}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:23, 100.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 36.44472122192383\n",
      "ppo/returns/mean: -0.966327428817749\n",
      "ppo/policy/advantages_mean: -6.0313798400102314e-09\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'hate', 'score': 0.09352150559425354}, {'label': 'nothate', 'score': -0.22506117820739746}], [{'label': 'nothate', 'score': 2.2582011222839355}, {'label': 'hate', 'score': -1.7329121828079224}], [{'label': 'nothate', 'score': 1.4088160991668701}, {'label': 'hate', 'score': -1.1574993133544922}], [{'label': 'nothate', 'score': 1.158969521522522}, {'label': 'hate', 'score': -0.938805341720581}], [{'label': 'nothate', 'score': 3.097087860107422}, {'label': 'hate', 'score': -2.486356496810913}], [{'label': 'nothate', 'score': 0.6831753849983215}, {'label': 'hate', 'score': -0.5990680456161499}], [{'label': 'nothate', 'score': 3.1910274028778076}, {'label': 'hate', 'score': -2.501859664916992}], [{'label': 'nothate', 'score': 2.4501819610595703}, {'label': 'hate', 'score': -1.9038281440734863}], [{'label': 'nothate', 'score': 1.4664907455444336}, {'label': 'hate', 'score': -1.1391825675964355}], [{'label': 'nothate', 'score': 2.4567790031433105}, {'label': 'hate', 'score': -1.888333797454834}], [{'label': 'nothate', 'score': 2.6652605533599854}, {'label': 'hate', 'score': -2.1171751022338867}], [{'label': 'nothate', 'score': 2.7537732124328613}, {'label': 'hate', 'score': -2.129239559173584}], [{'label': 'nothate', 'score': 2.300264358520508}, {'label': 'hate', 'score': -1.8309781551361084}], [{'label': 'nothate', 'score': 1.0939995050430298}, {'label': 'hate', 'score': -0.8754607439041138}], [{'label': 'nothate', 'score': 2.8754048347473145}, {'label': 'hate', 'score': -2.270282745361328}], [{'label': 'nothate', 'score': 1.2413618564605713}, {'label': 'hate', 'score': -1.0176215171813965}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [04:47, 93.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.097347259521484\n",
      "ppo/returns/mean: -0.9715116024017334\n",
      "ppo/policy/advantages_mean: -8.30156832165585e-09\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 3.282665252685547}, {'label': 'hate', 'score': -2.659043312072754}], [{'label': 'nothate', 'score': 1.8448354005813599}, {'label': 'hate', 'score': -1.445022702217102}], [{'label': 'nothate', 'score': 1.9774035215377808}, {'label': 'hate', 'score': -1.5677027702331543}], [{'label': 'nothate', 'score': 3.7356529235839844}, {'label': 'hate', 'score': -3.0938377380371094}], [{'label': 'nothate', 'score': 3.152747392654419}, {'label': 'hate', 'score': -2.568631410598755}], [{'label': 'nothate', 'score': 1.694312334060669}, {'label': 'hate', 'score': -1.3282610177993774}], [{'label': 'nothate', 'score': 1.6597055196762085}, {'label': 'hate', 'score': -1.3082761764526367}], [{'label': 'nothate', 'score': 1.6032626628875732}, {'label': 'hate', 'score': -1.224853515625}], [{'label': 'nothate', 'score': 2.03071665763855}, {'label': 'hate', 'score': -1.5788406133651733}], [{'label': 'nothate', 'score': 1.5780155658721924}, {'label': 'hate', 'score': -1.306355595588684}], [{'label': 'nothate', 'score': 2.194727659225464}, {'label': 'hate', 'score': -1.649925947189331}], [{'label': 'nothate', 'score': 0.9276605844497681}, {'label': 'hate', 'score': -0.7546229362487793}], [{'label': 'nothate', 'score': 3.1150155067443848}, {'label': 'hate', 'score': -2.470597743988037}], [{'label': 'nothate', 'score': 1.2203640937805176}, {'label': 'hate', 'score': -0.9809522032737732}], [{'label': 'nothate', 'score': 2.667938232421875}, {'label': 'hate', 'score': -2.0978496074676514}], [{'label': 'nothate', 'score': 1.4812806844711304}, {'label': 'hate', 'score': -1.1502442359924316}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [06:08, 88.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.190887451171875\n",
      "ppo/returns/mean: -0.7808466553688049\n",
      "ppo/policy/advantages_mean: -4.3584846842747993e-10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 2.0312414169311523}, {'label': 'hate', 'score': -1.6007598638534546}], [{'label': 'nothate', 'score': 2.7916512489318848}, {'label': 'hate', 'score': -2.205249786376953}], [{'label': 'nothate', 'score': 2.4140689373016357}, {'label': 'hate', 'score': -1.8804314136505127}], [{'label': 'nothate', 'score': 3.057588815689087}, {'label': 'hate', 'score': -2.369279146194458}], [{'label': 'nothate', 'score': 3.401616096496582}, {'label': 'hate', 'score': -2.7510058879852295}], [{'label': 'nothate', 'score': 1.4757474660873413}, {'label': 'hate', 'score': -1.1663193702697754}], [{'label': 'nothate', 'score': 2.470313549041748}, {'label': 'hate', 'score': -1.9726637601852417}], [{'label': 'nothate', 'score': 0.9484515190124512}, {'label': 'hate', 'score': -0.79299396276474}], [{'label': 'nothate', 'score': 2.458570957183838}, {'label': 'hate', 'score': -1.9306120872497559}], [{'label': 'nothate', 'score': 3.198556423187256}, {'label': 'hate', 'score': -2.571500778198242}], [{'label': 'nothate', 'score': 2.646561622619629}, {'label': 'hate', 'score': -2.046788215637207}], [{'label': 'nothate', 'score': 3.041642189025879}, {'label': 'hate', 'score': -2.4325456619262695}], [{'label': 'nothate', 'score': 0.994414210319519}, {'label': 'hate', 'score': -0.8188378810882568}], [{'label': 'nothate', 'score': 2.6619133949279785}, {'label': 'hate', 'score': -2.0876107215881348}], [{'label': 'nothate', 'score': 2.361302375793457}, {'label': 'hate', 'score': -1.8405518531799316}], [{'label': 'nothate', 'score': 2.2430577278137207}, {'label': 'hate', 'score': -1.747748851776123}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [07:30, 85.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 27.19145965576172\n",
      "ppo/returns/mean: -0.5293272733688354\n",
      "ppo/policy/advantages_mean: -8.731987577448308e-09\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 1.9366639852523804}, {'label': 'hate', 'score': -1.4840123653411865}], [{'label': 'nothate', 'score': 2.6126298904418945}, {'label': 'hate', 'score': -2.046844482421875}], [{'label': 'nothate', 'score': 2.418452501296997}, {'label': 'hate', 'score': -1.8784294128417969}], [{'label': 'nothate', 'score': 1.57919442653656}, {'label': 'hate', 'score': -1.2508587837219238}], [{'label': 'nothate', 'score': 2.963106632232666}, {'label': 'hate', 'score': -2.4120545387268066}], [{'label': 'nothate', 'score': 2.3630409240722656}, {'label': 'hate', 'score': -1.8591077327728271}], [{'label': 'nothate', 'score': 2.8167941570281982}, {'label': 'hate', 'score': -2.1615700721740723}], [{'label': 'nothate', 'score': 2.5310134887695312}, {'label': 'hate', 'score': -1.997727632522583}], [{'label': 'nothate', 'score': 1.665323257446289}, {'label': 'hate', 'score': -1.2828097343444824}], [{'label': 'nothate', 'score': 2.2883076667785645}, {'label': 'hate', 'score': -1.7741165161132812}], [{'label': 'nothate', 'score': 2.35921049118042}, {'label': 'hate', 'score': -1.8528707027435303}], [{'label': 'nothate', 'score': 1.9875316619873047}, {'label': 'hate', 'score': -1.5397799015045166}], [{'label': 'nothate', 'score': 1.937908411026001}, {'label': 'hate', 'score': -1.6555025577545166}], [{'label': 'nothate', 'score': 2.679774761199951}, {'label': 'hate', 'score': -2.0704290866851807}], [{'label': 'nothate', 'score': 2.2151107788085938}, {'label': 'hate', 'score': -1.7168283462524414}], [{'label': 'nothate', 'score': 2.421614646911621}, {'label': 'hate', 'score': -1.8670109510421753}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [09:05, 89.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 31.77707290649414\n",
      "ppo/returns/mean: -0.8464198112487793\n",
      "ppo/policy/advantages_mean: -2.0219784957475895e-08\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 1.41933274269104}, {'label': 'hate', 'score': -1.1345998048782349}], [{'label': 'nothate', 'score': 3.4381227493286133}, {'label': 'hate', 'score': -2.8685507774353027}], [{'label': 'nothate', 'score': 0.5207525491714478}, {'label': 'hate', 'score': -0.4411659836769104}], [{'label': 'nothate', 'score': 2.7762651443481445}, {'label': 'hate', 'score': -2.2506656646728516}], [{'label': 'nothate', 'score': 2.150848150253296}, {'label': 'hate', 'score': -1.6764607429504395}], [{'label': 'nothate', 'score': 3.277405023574829}, {'label': 'hate', 'score': -2.651092529296875}], [{'label': 'nothate', 'score': 2.4621965885162354}, {'label': 'hate', 'score': -1.9132494926452637}], [{'label': 'nothate', 'score': 1.9966607093811035}, {'label': 'hate', 'score': -1.5378072261810303}], [{'label': 'nothate', 'score': 2.808950424194336}, {'label': 'hate', 'score': -2.2453293800354004}], [{'label': 'nothate', 'score': 2.9801554679870605}, {'label': 'hate', 'score': -2.351010322570801}], [{'label': 'nothate', 'score': 2.3189644813537598}, {'label': 'hate', 'score': -1.8521826267242432}], [{'label': 'nothate', 'score': 2.353762149810791}, {'label': 'hate', 'score': -1.813701868057251}], [{'label': 'nothate', 'score': 3.213533401489258}, {'label': 'hate', 'score': -2.529445171356201}], [{'label': 'nothate', 'score': 2.1044116020202637}, {'label': 'hate', 'score': -1.6501400470733643}], [{'label': 'nothate', 'score': 2.3598709106445312}, {'label': 'hate', 'score': -1.8664475679397583}], [{'label': 'nothate', 'score': 1.6259771585464478}, {'label': 'hate', 'score': -1.276881217956543}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [10:40, 90.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 28.974096298217773\n",
      "ppo/returns/mean: -0.6000247597694397\n",
      "ppo/policy/advantages_mean: -2.4664952302799747e-09\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 2.459815502166748}, {'label': 'hate', 'score': -1.9132345914840698}], [{'label': 'nothate', 'score': 1.080428957939148}, {'label': 'hate', 'score': -0.8835965991020203}], [{'label': 'nothate', 'score': 3.2355856895446777}, {'label': 'hate', 'score': -2.6272566318511963}], [{'label': 'nothate', 'score': 3.628333568572998}, {'label': 'hate', 'score': -2.9806172847747803}], [{'label': 'nothate', 'score': 3.2258081436157227}, {'label': 'hate', 'score': -2.571758270263672}], [{'label': 'nothate', 'score': 2.270799398422241}, {'label': 'hate', 'score': -1.7947824001312256}], [{'label': 'nothate', 'score': 2.269404411315918}, {'label': 'hate', 'score': -1.7373530864715576}], [{'label': 'nothate', 'score': 2.244831085205078}, {'label': 'hate', 'score': -1.7553808689117432}], [{'label': 'nothate', 'score': 3.350450038909912}, {'label': 'hate', 'score': -2.679737091064453}], [{'label': 'nothate', 'score': 2.7296504974365234}, {'label': 'hate', 'score': -2.092463493347168}], [{'label': 'nothate', 'score': 3.008242607116699}, {'label': 'hate', 'score': -2.411439895629883}], [{'label': 'nothate', 'score': 2.287853717803955}, {'label': 'hate', 'score': -1.7838003635406494}], [{'label': 'nothate', 'score': 0.9198544025421143}, {'label': 'hate', 'score': -0.7112827301025391}], [{'label': 'nothate', 'score': 0.8785864114761353}, {'label': 'hate', 'score': -0.7551174163818359}], [{'label': 'nothate', 'score': 1.7794626951217651}, {'label': 'hate', 'score': -1.3823118209838867}], [{'label': 'nothate', 'score': 3.3617074489593506}, {'label': 'hate', 'score': -2.7473182678222656}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [15:15, 91.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 33.04023361206055\n",
      "ppo/returns/mean: -0.933142364025116\n",
      "ppo/policy/advantages_mean: 1.2839547203213897e-08\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[{'label': 'nothate', 'score': 2.025411605834961}, {'label': 'hate', 'score': -1.5545072555541992}], [{'label': 'nothate', 'score': 2.707101821899414}, {'label': 'hate', 'score': -2.1036617755889893}], [{'label': 'nothate', 'score': 2.295156955718994}, {'label': 'hate', 'score': -1.7623037099838257}], [{'label': 'nothate', 'score': 2.160024642944336}, {'label': 'hate', 'score': -1.7335741519927979}], [{'label': 'nothate', 'score': 2.1263046264648438}, {'label': 'hate', 'score': -1.6454317569732666}], [{'label': 'nothate', 'score': 1.5389256477355957}, {'label': 'hate', 'score': -1.2253371477127075}], [{'label': 'nothate', 'score': 2.1456289291381836}, {'label': 'hate', 'score': -1.707491159439087}], [{'label': 'nothate', 'score': 2.521397590637207}, {'label': 'hate', 'score': -1.9626708030700684}], [{'label': 'nothate', 'score': 2.814171314239502}, {'label': 'hate', 'score': -2.2026593685150146}], [{'label': 'nothate', 'score': 3.679547071456909}, {'label': 'hate', 'score': -3.0283737182617188}], [{'label': 'nothate', 'score': 2.0709846019744873}, {'label': 'hate', 'score': -1.602299451828003}], [{'label': 'nothate', 'score': 0.9182411432266235}, {'label': 'hate', 'score': -0.7495813369750977}], [{'label': 'nothate', 'score': 1.9426121711730957}, {'label': 'hate', 'score': -1.4811203479766846}], [{'label': 'nothate', 'score': 1.9092576503753662}, {'label': 'hate', 'score': -1.4777345657348633}], [{'label': 'nothate', 'score': 0.868461012840271}, {'label': 'hate', 'score': -0.6957281231880188}], [{'label': 'nothate', 'score': 2.2025551795959473}, {'label': 'hate', 'score': -1.7263352870941162}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [16:52, 92.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 32.764068603515625\n",
      "ppo/returns/mean: -0.9268460273742676\n",
      "ppo/policy/advantages_mean: -1.7478838376661088e-09\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_min_len = 100\n",
    "output_max_len = 400\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "generation_kwards = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwards = {\n",
    "    \"top_k\": None,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # break when reach the max_steps\n",
    "    if step > max_ppo_steps:\n",
    "        break\n",
    "    \n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "    # Get responses from FLAN-T5/PEFT LLM\n",
    "    summary_tensors = []\n",
    "    \n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "        \n",
    "        generation_kwards[\"max_new_tokens\"] = max_new_tokens\n",
    "        \n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwards)\n",
    "        \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "        \n",
    "        \n",
    "    # This need to be called response\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "        \n",
    "    # Compute rewards output\n",
    "    query_reponse_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_reponse_pairs, **reward_kwards)   \n",
    "    \n",
    "    print(rewards)\n",
    "    # Use the [nothate] item because this is the score for the position [nothate] class\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
    "                                  \n",
    "    # Run ppo step\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9c4b6-6d38-4a47-ac35-eb6ebde7ed24",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "3.3 - Avalia o modelo quantitativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54d4c4aa-b976-414d-946a-6196f5b77bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:19,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity mean and std after fine-tunning, mean: 0.014201556380033831 - std: 0.01997620393343237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_fine_tunning, std_after_fine_tunning = evaluate_toxicity(model=ppo_model,\n",
    "                                                                      toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                      tokenizer=tokenizer,\n",
    "                                                                      dataset=dataset[\"test\"],\n",
    "                                                                      num_samples=10)\n",
    "\n",
    "print(f\"toxicity mean and std after fine-tunning, mean: {mean_after_fine_tunning} - std: {std_after_fine_tunning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41306bc7-2466-4d8a-abed-3b686fb5ae87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean improvement of the ppo from the base model: -0.24510890838287297\n",
      "Std improvement of the ppo from the base model: -0.10990024085940953\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_after_fine_tunning - mean_before_fine_tunning) / mean_after_fine_tunning\n",
    "\n",
    "std_improvement = (std_after_fine_tunning - std_before_fine_tunning) / std_after_fine_tunning\n",
    "\n",
    "print(f\"Mean improvement of the ppo from the base model: {mean_improvement}\")\n",
    "print(f\"Std improvement of the ppo from the base model: {std_improvement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195328e-c8f7-4bce-bc5c-fdaca0b1ae0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## 3.3 - Avalia o modelo qualitativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0965aa63-c0cc-491e-a913-e2016f8fd674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0: batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "\n",
    "summary_tensors = []\n",
    "\n",
    "generation_kwards = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "# Get response from ppo base model\n",
    "\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwards[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwards,\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "        \n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwards,\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode Responses\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "              \n",
    "# Sentiment analysis fom query/response pairs before/after\n",
    "text_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewords_before = sentiment_pipe(text_before, **reward_kwards)\n",
    "compare_results[\"sentiment_before\"] = [reward[not_hate_index][\"score\"] for reward in rewords_before]\n",
    "\n",
    "text_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewords_after = sentiment_pipe(text_after, **reward_kwards)\n",
    "compare_results[\"sentiment_after\"] = [reward[not_hate_index][\"score\"] for reward in rewords_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70dcc3c-2344-4033-9d91-abba013db2a2",
   "metadata": {},
   "source": [
    "Guarda e reve os resultados em um dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03c60c93-a3c2-4d91-85b8-62437a2e397d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>sentiment_before</th>\n",
       "      <th>sentiment_after</th>\n",
       "      <th>rewards_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy is surprised that Richard was fired from a team. Judy is also surprised.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and #Person1# are surprised because Richard was fired by our manager.&lt;/s&gt;</td>\n",
       "      <td>1.137102</td>\n",
       "      <td>2.143000</td>\n",
       "      <td>1.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# how to buy goods through the web. #Person1# shows how it is done for a customer without going to the physical stores. #Person1# tells #Person2# how it is done.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# highly recommends the use of PCs to help people communicate with the outside world more. Can people buy goods through it without going out to the physical stores?&lt;/s&gt;</td>\n",
       "      <td>2.426362</td>\n",
       "      <td>3.033279</td>\n",
       "      <td>0.606918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1#, #Person2# says her flight got in 15 minutes ago and misses hers.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1#'s flight got in 15 minutes ago. #Person2# will go and find out if there is more.&lt;/s&gt;</td>\n",
       "      <td>1.926300</td>\n",
       "      <td>2.511166</td>\n",
       "      <td>0.584867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...</td>\n",
       "      <td>&lt;pad&gt; #Person1# is forming a music band and talks about the members of the band. #Person2# will audition #Person1#'s country singing talent with the help of #Person1#'s house for audition.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# to audition for a rock band. #Person1# talks about the members of the band and calls them to audition. #Person1# refuses because #Person2# doesn't have enough room for the amplifiers, microphones or even the drums.&lt;/s&gt;</td>\n",
       "      <td>2.406677</td>\n",
       "      <td>2.875031</td>\n",
       "      <td>0.468354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
       "      <td>&lt;pad&gt; #Person2# looks at the flesh of a final draft of the contract. She wants to sign the contract right now. #Person1# advises her to plenty of time in order to check over the draft.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# and #Person2# are happy they reached an agreement on almost every term in their trade. To ask some questions, #Person1# presents the final draft.&lt;/s&gt;</td>\n",
       "      <td>2.867878</td>\n",
       "      <td>3.231239</td>\n",
       "      <td>0.363361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
       "      <td>&lt;pad&gt; Allen tells #Person2# he is blind for the winter because he broke into the house with the help of the blind help. Inhuman underwear and stereo aren't found at all. He also tells #Person1# that the unlocked window is open.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Allen and #Person2# tell each other the window was broken and robber wants to find someone. Allen and #Person1# will look upstairs to find someone.&lt;/s&gt;</td>\n",
       "      <td>1.591679</td>\n",
       "      <td>1.878596</td>\n",
       "      <td>0.286917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# has most of the toy cars but the others are too expensive. They show #Person2# the other one and they both agree. #Person2# accepts and gets in.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# helps #Person2# find the cheaper toy car to get for #Person2# and #Person2# likes it.&lt;/s&gt;</td>\n",
       "      <td>1.407550</td>\n",
       "      <td>1.635844</td>\n",
       "      <td>0.228295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
       "      <td>&lt;pad&gt; #Person2# recommends dial-up to #Person1# for a high-speed internet starting at DEL instead of dial-up because it doesn't tie up the phone. But #Person1# can't use @ the Internet if #Person1#'s on the internet.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# would like to order Kplinter internet. #Person1# says DEL isn't connected through the phone network but Dial-up.&lt;/s&gt;</td>\n",
       "      <td>2.451935</td>\n",
       "      <td>2.564701</td>\n",
       "      <td>0.112766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# helps #Person1# crochet a cash. #Person1# wants to cash in 10 hundreds and 10 twenties, and only in small change.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to send a cashed cheque to #Person2#. #Person1# tells #Person2# the amount in cash.&lt;/s&gt;</td>\n",
       "      <td>1.647519</td>\n",
       "      <td>1.753495</td>\n",
       "      <td>0.105976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# #Person1# is confused about a job. #Person2# offers advice on who can come help #Person1#.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# thinks it convenient to work in an office. #Person2# will help #Person1# to help #Person1# find a job. #Person1# turns to #Person2#'s assistant to help #Person1# find a job.&lt;/s&gt;</td>\n",
       "      <td>1.993768</td>\n",
       "      <td>2.070232</td>\n",
       "      <td>0.076465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
       "      <td>&lt;pad&gt; #Person1# smells like an ashtray and says they don't agree that they will go cold turkey overnight. No one says there are other ways to fight with the urge to get too hot for cigarettes. Fortunately, it's Honey's goal to quit smoking. She's trying but could run out of willpower.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# has a look like an ashtray and shows that #Person2# hasn't agreed to meet him to quit smoking. #Person1# advises her to turn to their services and changes in new public places, she refuses.&lt;/s&gt;</td>\n",
       "      <td>1.614836</td>\n",
       "      <td>1.577484</td>\n",
       "      <td>-0.037352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
       "      <td>&lt;pad&gt; #Person1# requests #Person2#'s registration card and to make a medical record. #Person2# tells #Person1# how to get to the consultant room by driving through the pharmacy or making a left turn until you come to the drugstore. Then #Person2# guides #Person1# to the camera room and explains how to get it.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# needs to register for the aferment. #Person2# asks #Person1# to pay for the registration money and tells #Person1# the how to get to the consulting room.&lt;/s&gt;</td>\n",
       "      <td>1.840296</td>\n",
       "      <td>1.702615</td>\n",
       "      <td>-0.137681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# #Person1#'s flight is delayed because #Person1# couldn't communicate with main airlines in English and #Person2# has a phone number so #Person1# asks to dial 35.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# for information about the actual flight number to London and #Person1# wants #Person2# to relay it to #Person1#. They both advise to dial 35.&lt;/s&gt;</td>\n",
       "      <td>1.911153</td>\n",
       "      <td>1.763365</td>\n",
       "      <td>-0.147788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# offers #Person2#$ 150 yuan a piece of bamboo can be bought at a minimum price with a 10 % volume discount. #Person2# accepts the offer.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to buy an online shop with a volume discount. #Person2# offers 1 000 yuan price and 10% discount to #Person1#.&lt;/s&gt;</td>\n",
       "      <td>2.817250</td>\n",
       "      <td>2.644486</td>\n",
       "      <td>-0.172763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
       "      <td>&lt;pad&gt; #Person1# invites #Person2# to take a coffee break. #Person2# will some time after work to make your work a little more while #Person2# wants to finish the report and #Person2# thinks it would be better to take a break.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to take a coffee break because #Person2# can't take a break from work. They agree to take a break though.&lt;/s&gt;</td>\n",
       "      <td>1.959503</td>\n",
       "      <td>1.757654</td>\n",
       "      <td>-0.201849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# is amazed by a concept the mom taught her. #Person1#'s mother is very proud that #Person1# worked hard. Their teacher agrees.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# needs proofread the paper for her, but she could lot if she will say it was worth the time.&lt;/s&gt;</td>\n",
       "      <td>2.687650</td>\n",
       "      <td>2.382229</td>\n",
       "      <td>-0.305420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# about the restaurant and asks about the food. #Person2# says the service was not good, but #Person2# likes the restaurant. #Person1# keeps talking about how unpopular the restaurant is but #Person2# doesn't buy the restaurant any more.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# complains about the restaurant and it seems to be a new one and the staff was not nice. #Person2# won't try this restaurant again.&lt;/s&gt;</td>\n",
       "      <td>1.993927</td>\n",
       "      <td>1.631404</td>\n",
       "      <td>-0.362523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# the way to cross bakery. #Person1# turns around the same way and goes three blocks to Broadway. #Person1# finds the cross bakery building on his left hand side. #Person2# shows #Person1# the route.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# the directions to cross the Cross Bakery building. #Person1# then tells #Person2# what to do and why to turn around, thus helping #Person1# to see the Bakery on the other side.&lt;/s&gt;</td>\n",
       "      <td>3.244104</td>\n",
       "      <td>2.809350</td>\n",
       "      <td>-0.434755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
       "      <td>&lt;pad&gt; Li Hong willn't go to see Mrs. Brown tomorrow morning because Alice's mother is ill so they'll visit Mrs. Brown later.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Alice has a ill mother. Li Hongmment Alice and asks Alice to stay at home before going to visit Mrs. Brown.&lt;/s&gt;</td>\n",
       "      <td>1.872572</td>\n",
       "      <td>1.391196</td>\n",
       "      <td>-0.481376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summaryze the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary:&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# thinks Amanda prefers the peaked cap and wants to buy a top hat. Amanda isn't interested in caps. #Person2# asks Amanda more questions about the sombrero in black.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda likes this peaked cap because it fits her and she doesn't like caps.&lt;/s&gt;</td>\n",
       "      <td>1.619195</td>\n",
       "      <td>1.088764</td>\n",
       "      <td>-0.530431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0                                                                                                                                                                    Summaryze the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary:</s>   \n",
       "1   Summaryze the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
       "2                                                                                                                                                                                                                                          Summaryze the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary:</s>   \n",
       "3   Summaryze the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...   \n",
       "4   Summaryze the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
       "5   Summaryze the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
       "6            Summaryze the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary:</s>   \n",
       "7   Summaryze the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
       "8                                                                                                                                                                          Summaryze the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary:</s>   \n",
       "9   Summaryze the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
       "10  Summaryze the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
       "11  Summaryze the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
       "12  Summaryze the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
       "13                                                                                     Summaryze the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary:</s>   \n",
       "14  Summaryze the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
       "15                       Summaryze the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary:</s>   \n",
       "16  Summaryze the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
       "17  Summaryze the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
       "18  Summaryze the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
       "19                                                                                                                                                                                                                           Summaryze the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary:</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                               response_before  \\\n",
       "0                                                                                                                                                                                                                                      <pad> Judy is surprised that Richard was fired from a team. Judy is also surprised.</s>   \n",
       "1                                                                                                                          <pad> #Person1# tells #Person2# how to buy goods through the web. #Person1# shows how it is done for a customer without going to the physical stores. #Person1# tells #Person2# how it is done.</s>   \n",
       "2                                                                                                                                                                                                                                        <pad> #Person1#, #Person2# says her flight got in 15 minutes ago and misses hers.</s>   \n",
       "3                                                                                                                             <pad> #Person1# is forming a music band and talks about the members of the band. #Person2# will audition #Person1#'s country singing talent with the help of #Person1#'s house for audition.</s>   \n",
       "4                                                                                                                                 <pad> #Person2# looks at the flesh of a final draft of the contract. She wants to sign the contract right now. #Person1# advises her to plenty of time in order to check over the draft.</s>   \n",
       "5                                                                                      <pad> Allen tells #Person2# he is blind for the winter because he broke into the house with the help of the blind help. Inhuman underwear and stereo aren't found at all. He also tells #Person1# that the unlocked window is open.</s>   \n",
       "6                                                                                                                                                         <pad> #Person1# has most of the toy cars but the others are too expensive. They show #Person2# the other one and they both agree. #Person2# accepts and gets in.</s>   \n",
       "7                                                                                                 <pad> #Person2# recommends dial-up to #Person1# for a high-speed internet starting at DEL instead of dial-up because it doesn't tie up the phone. But #Person1# can't use @ the Internet if #Person1#'s on the internet.</s>   \n",
       "8                                                                                                                                                                                        <pad> #Person2# helps #Person1# crochet a cash. #Person1# wants to cash in 10 hundreds and 10 twenties, and only in small change.</s>   \n",
       "9                                                                                                                                                                                               <pad> #Person1# tells #Person2# #Person1# is confused about a job. #Person2# offers advice on who can come help #Person1#.</s>   \n",
       "10                           <pad> #Person1# smells like an ashtray and says they don't agree that they will go cold turkey overnight. No one says there are other ways to fight with the urge to get too hot for cigarettes. Fortunately, it's Honey's goal to quit smoking. She's trying but could run out of willpower.</s>   \n",
       "11  <pad> #Person1# requests #Person2#'s registration card and to make a medical record. #Person2# tells #Person1# how to get to the consultant room by driving through the pharmacy or making a left turn until you come to the drugstore. Then #Person2# guides #Person1# to the camera room and explains how to get it.</s>   \n",
       "12                                                                                                                       <pad> #Person1# tells #Person2# #Person1#'s flight is delayed because #Person1# couldn't communicate with main airlines in English and #Person2# has a phone number so #Person1# asks to dial 35.</s>   \n",
       "13                                                                                                                                                                 <pad> #Person1# offers #Person2#$ 150 yuan a piece of bamboo can be bought at a minimum price with a 10 % volume discount. #Person2# accepts the offer.</s>   \n",
       "14                                                                                       <pad> #Person1# invites #Person2# to take a coffee break. #Person2# will some time after work to make your work a little more while #Person2# wants to finish the report and #Person2# thinks it would be better to take a break.</s>   \n",
       "15                                                                                                                                                                           <pad> #Person1# is amazed by a concept the mom taught her. #Person1#'s mother is very proud that #Person1# worked hard. Their teacher agrees.</s>   \n",
       "16                                              <pad> #Person1# asks #Person2# about the restaurant and asks about the food. #Person2# says the service was not good, but #Person2# likes the restaurant. #Person1# keeps talking about how unpopular the restaurant is but #Person2# doesn't buy the restaurant any more.</s>   \n",
       "17                                                                                   <pad> #Person2# tells #Person1# the way to cross bakery. #Person1# turns around the same way and goes three blocks to Broadway. #Person1# finds the cross bakery building on his left hand side. #Person2# shows #Person1# the route.</s>   \n",
       "18                                                                                                                                                                                            <pad> Li Hong willn't go to see Mrs. Brown tomorrow morning because Alice's mother is ill so they'll visit Mrs. Brown later.</s>   \n",
       "19                                                                                                                                     <pad> #Person2# thinks Amanda prefers the peaked cap and wants to buy a top hat. Amanda isn't interested in caps. #Person2# asks Amanda more questions about the sombrero in black.</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                               response_after  \\\n",
       "0                                                                                                                                                                        <pad> Judy and #Person1# are surprised because Richard was fired by our manager.</s>   \n",
       "1                                                                      <pad> #Person1# highly recommends the use of PCs to help people communicate with the outside world more. Can people buy goods through it without going out to the physical stores?</s>   \n",
       "2                                                                                                                                                        <pad> #Person1#'s flight got in 15 minutes ago. #Person2# will go and find out if there is more.</s>   \n",
       "3   <pad> #Person1# asks #Person2# to audition for a rock band. #Person1# talks about the members of the band and calls them to audition. #Person1# refuses because #Person2# doesn't have enough room for the amplifiers, microphones or even the drums.</s>   \n",
       "4                                                                                       <pad> #Person1# and #Person2# are happy they reached an agreement on almost every term in their trade. To ask some questions, #Person1# presents the final draft.</s>   \n",
       "5                                                                                               <pad> Allen and #Person2# tell each other the window was broken and robber wants to find someone. Allen and #Person1# will look upstairs to find someone.</s>   \n",
       "6                                                                                                                                                   <pad> #Person1# helps #Person2# find the cheaper toy car to get for #Person2# and #Person2# likes it.</s>   \n",
       "7                                                                                                                        <pad> #Person1# would like to order Kplinter internet. #Person1# says DEL isn't connected through the phone network but Dial-up.</s>   \n",
       "8                                                                                                                                               <pad> #Person1# wants to send a cashed cheque to #Person2#. #Person1# tells #Person2# the amount in cash.</s>   \n",
       "9                                                           <pad> #Person1# thinks it convenient to work in an office. #Person2# will help #Person1# to help #Person1# find a job. #Person1# turns to #Person2#'s assistant to help #Person1# find a job.</s>   \n",
       "10                                          <pad> #Person1# has a look like an ashtray and shows that #Person2# hasn't agreed to meet him to quit smoking. #Person1# advises her to turn to their services and changes in new public places, she refuses.</s>   \n",
       "11                                                                              <pad> #Person1# needs to register for the aferment. #Person2# asks #Person1# to pay for the registration money and tells #Person1# the how to get to the consulting room.</s>   \n",
       "12                                                                           <pad> #Person1# asks #Person2# for information about the actual flight number to London and #Person1# wants #Person2# to relay it to #Person1#. They both advise to dial 35.</s>   \n",
       "13                                                                                                                   <pad> #Person1# wants to buy an online shop with a volume discount. #Person2# offers 1 000 yuan price and 10% discount to #Person1#.</s>   \n",
       "14                                                                                                                        <pad> #Person1# wants to take a coffee break because #Person2# can't take a break from work. They agree to take a break though.</s>   \n",
       "15                                                                                                                                            <pad> #Person1# needs proofread the paper for her, but she could lot if she will say it was worth the time.</s>   \n",
       "16                                                                                                     <pad> #Person2# complains about the restaurant and it seems to be a new one and the staff was not nice. #Person2# won't try this restaurant again.</s>   \n",
       "17                                       <pad> #Person2# tells #Person1# the directions to cross the Cross Bakery building. #Person1# then tells #Person2# what to do and why to turn around, thus helping #Person1# to see the Bakery on the other side.</s>   \n",
       "18                                                                                                                                      <pad> Alice has a ill mother. Li Hongmment Alice and asks Alice to stay at home before going to visit Mrs. Brown.</s>   \n",
       "19                                                                                                                                                                      <pad> Amanda likes this peaked cap because it fits her and she doesn't like caps.</s>   \n",
       "\n",
       "    sentiment_before  sentiment_after  rewards_diff  \n",
       "0           1.137102         2.143000      1.005898  \n",
       "1           2.426362         3.033279      0.606918  \n",
       "2           1.926300         2.511166      0.584867  \n",
       "3           2.406677         2.875031      0.468354  \n",
       "4           2.867878         3.231239      0.363361  \n",
       "5           1.591679         1.878596      0.286917  \n",
       "6           1.407550         1.635844      0.228295  \n",
       "7           2.451935         2.564701      0.112766  \n",
       "8           1.647519         1.753495      0.105976  \n",
       "9           1.993768         2.070232      0.076465  \n",
       "10          1.614836         1.577484     -0.037352  \n",
       "11          1.840296         1.702615     -0.137681  \n",
       "12          1.911153         1.763365     -0.147788  \n",
       "13          2.817250         2.644486     -0.172763  \n",
       "14          1.959503         1.757654     -0.201849  \n",
       "15          2.687650         2.382229     -0.305420  \n",
       "16          1.993927         1.631404     -0.362523  \n",
       "17          3.244104         2.809350     -0.434755  \n",
       "18          1.872572         1.391196     -0.481376  \n",
       "19          1.619195         1.088764     -0.530431  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"rewards_diff\"] = df_compare_results[\"sentiment_after\"] - df_compare_results[\"sentiment_before\"]\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=\"rewards_diff\", ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda459e-3101-4362-86d8-6593704080f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
