{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01041596-c02e-4bcd-a109-cf32d3d27301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Objective:\n",
    "\n",
    "# Este notebook tem o objetivo de rodar um algoritmo que realiza o resumo de um texto que lhe é apresentado.\n",
    "# Exemplo 1: \n",
    "# Texto (entrada): Hoje o dia esta ensolarado e é necessarios, dado que o calor pode resercar as plantas, molha-las.\n",
    "# Resumo (output): O dia está ensolarado, é necessário molhas as plantas.\n",
    "# Exemplo 2: \n",
    "# Texto (entrada): Ontem tivemos que sair mais tarde, porque havia muito trabalho a ser feito no escritório. Com isso, levando em consideracao que pegamos transito no caminho de volta para casa, \n",
    "# chegamos após o horário do jantar.\n",
    "# Resumo (output): Ontem, devido ao volume de trabalho, chegamos após o horário do jantar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fec37cb-5371-426f-b20b-0507b564b042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected instance: ml-m5-2xlarge\n",
      "current instance: instance-datascience-ml-m5-2xlarge\n",
      "Instance type has been choose correctly\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "instance_type_expected = 'ml-m5-2xlarge'\n",
    "instance_type_current = os.environ.get('HOSTNAME')\n",
    "\n",
    "print('expected instance:', instance_type_expected)\n",
    "print('current instance:', instance_type_current)\n",
    "\n",
    "assert instance_type_expected in instance_type_current, f'Error: you selected the wrong instance, please select the correct before start'\n",
    "print('Instance type has been choose correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da263fc5-4cd6-4c12-843f-e61f7be60f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.17.0 in /opt/conda/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from datasets==2.17.0) (6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installing librarys\n",
    "! pip install -U datasets==2.17.0\n",
    "\n",
    "! pip install --upgrade pip\n",
    "! pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "!pip install \\\n",
    "    transformers==4.27.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee3e9d1-8ea4-4688-999f-6eac47fe5361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fba84e-ac76-45ff-9e8c-9eb3c0e07578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Summarizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82565695-82b7-477c-9749-1da4671a0225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hugingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(hugingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2664325a-2c32-44ff-b546-c8908ce949a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print some examples of dialogues in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eec1d95-9403-41e0-af2b-e27a14f0d4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "example 0\n",
      "input dialogue\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "baseline human summary\n",
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "example 50\n",
      "input dialogue\n",
      "#Person1#: You have the right to remain silent. Anything you say can and will be used against you in a court of law. You have the right to have an attorney present during questioning. If you cannot afford an attorney, one will be appointed for you. Do you understand?\n",
      "#Person2#: Yes.\n",
      "#Person1#: What's your name?\n",
      "#Person2#: My name is James.\n",
      "#Person1#: What's your nationality?\n",
      "#Person2#: American.\n",
      "#Person1#: What's your relationship with the victim?\n",
      "#Person2#: I don't know him.\n",
      "#Person1#: Why did you attack the victim?\n",
      "#Person2#: Because he beat me first when I tried to stop him from grabbing my bag and running away.\n",
      "#Person1#: How many times did you stab the victim?\n",
      "#Person2#: I stabbed his belly three times.\n",
      "#Person1#: Did you know that your actions might cause serous injuries or death?\n",
      "#Person2#: I knew, but I couldn't control myself.\n",
      "#Person1#: Was it your intention to kill the victim?\n",
      "#Person2#: No. I didn't kill him on purpose, madam. It's him who caused the incident. I need to see my attorney.\n",
      "#Person1#: OK. Give me his number and we'll contact him.\n",
      "baseline human summary\n",
      "#Person1# stabbed the victim because he beat #Person1# first and tried to grab #Person1#'s bag. #Person1# says he didn't kill him on purpose.\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 50]:\n",
    "    print('-'*100)\n",
    "    print('example', i)\n",
    "    print('input dialogue')\n",
    "    print(dataset['train']['dialogue'][i])\n",
    "    print('baseline human summary')\n",
    "    print(dataset['train']['summary'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01e2972-1b53-4526-b9c9-74287db0c7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdeeda449894098b8cd6660350c1fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e7d9a5813d4214ae51ee3186f210d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c604c81486c4eb1990ee21f1c0ec9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and create a instance. The model is a FLAN-T5\n",
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6e95cc-8ff3-46de-b65c-af853bcbf2f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b4f89a5dfa469ab0da008ef752d98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8afc878de54971bbf2ecb87072ddbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3bbfbf15f64b348fdae5aa3847dd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207d48dc5aad423596b51c0faded0883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a770939-75ab-4281-b2a2-fd6b260185f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sentence tensor([ 1713,   345, 13515,   536,  4663,    10,  2018,     6,  1363,     5,\n",
      "         3931,     5,    27,    31,    51,  7582, 12833,    77,     7,     5,\n",
      "         1615,    33,    25,   270,   469,    58,  1713,   345, 13515,   357,\n",
      "         4663,    10,    27,   435,    34,   133,    36,     3,     9,   207,\n",
      "          800,    12,   129,     3,     9,   691,    18,   413,     5,  1713,\n",
      "          345, 13515,   536,  4663,    10,  2163,     6,   168,     6,    25,\n",
      "           43,    29,    31,    17,   141,    80,    21,   305,   203,     5,\n",
      "          148,   225,    43,    80,   334,   215,     5,  1713,   345, 13515,\n",
      "          357,  4663,    10,    27,   214,     5,    27,  2320,    38,   307,\n",
      "           38,   132,    19,  1327,  1786,     6,   572,   281,   217,     8,\n",
      "         2472,    58,  1713,   345, 13515,   536,  4663,    10,  1548,     6,\n",
      "            8,   200,   194,    12,  1792,  2261, 21154,    19,    12,   253,\n",
      "           91,    81,   135,   778,     5,   264,   653,    12,   369,    44,\n",
      "          709,   728,     3,     9,   215,    21,    39,   293,   207,     5,\n",
      "         1713,   345, 13515,   357,  4663,    10,  8872,     5,  1713,   345,\n",
      "        13515,   536,  4663,    10,  1563,   140,   217,   270,     5,   696,\n",
      "         2053,    11, 11581,   320,  1399,     5,  2321,     3,     9,  1659,\n",
      "         6522,     6,   754,     5,   531,    25,  7269,     6,  1363,     5,\n",
      "         3931,    58,  1713,   345, 13515,   357,  4663,    10,  2163,     5,\n",
      "         1713,   345, 13515,   536,  4663,    10, 14627,    53,    19,     8,\n",
      "         1374,  1137,    13,  5084,  1874,    11,   842,  1994,     6,    25,\n",
      "          214,     5,   148,   310,   225, 10399,     5,  1713,   345, 13515,\n",
      "          357,  4663,    10,    27,    31,   162,  1971,  3986,    13,   648,\n",
      "            6,    68,    27,   131,    54,    31,    17,  1727,    12,  4583,\n",
      "            8,  7386,     5,  1713,   345, 13515,   536,  4663,    10,  1548,\n",
      "            6,    62,    43,  2287,    11,   128, 11208,    24,   429,   199,\n",
      "            5,    27,    31,   195,   428,    25,    72,   251,   274,    25,\n",
      "         1175,     5,  1713,   345, 13515,   357,  4663,    10,  8872,     6,\n",
      "         2049,  2472,     5,     1])\n",
      "Decoded sentence #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would be a good idea to get a check-up. #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year. #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor? #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good. #Person2#: Ok. #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith? #Person2#: Yes. #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit. #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit. #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave. #Person2#: Ok, thanks doctor.</s>\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer encode/decode\n",
    "test_phrase = dataset['train']['dialogue'][0]\n",
    "\n",
    "sentence_encoded = tokenizer(test_phrase, return_tensors='pt')\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0], skep_special_tokens=True)\n",
    "\n",
    "print('Encoded sentence', sentence_encoded['input_ids'][0])\n",
    "print('Decoded sentence', sentence_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcaffcfe-1b8e-4f23-a19f-1d54e3e6e90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Inputs:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "Outputs:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n"
     ]
    }
   ],
   "source": [
    "# Vamos agora testar como o modelo desempenho uma tarefa de previsao sem prompt_enginnering.\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "inputs = tokenizer(phrase, return_tensors='pt')    # Returning pythorch tensors\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('Example:')\n",
    "print(phrase)\n",
    "print('Inputs:')\n",
    "print(summary)\n",
    "print('Outputs:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8a762-6506-4404-88ae-ecbae803508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parece que o resumo realizado pelo modelo não vai de encontro com o dialogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac719f81-f2bf-4ad3-8568-4e45d4fb474b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Summarize Dialogue with the prompt enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "092db97c-e6e5-47cd-bb9c-978a0fc15da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.2 Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e159aa5-4523-4278-add6-222221fcb191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Inputs:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "Outputs:\n",
      "The following memo is to be distributed to all employees by this afternoon.\n"
     ]
    }
   ],
   "source": [
    "# Agora vamos passar para o modelo uma instrução pelo prompt para verificar o seu retorno, apos ser recebido o prompt\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "prompt = f\"\"\"Summarize the follow conversation.\n",
    "\n",
    "        {phrase}\n",
    "        \n",
    "        Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Vamos agora testar como o modelo desempenho uma tarefa de previsao sem prompt_enginnering.\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')    # Returning pythorch tensors\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('Example:')\n",
    "print(phrase)\n",
    "print('Summary:')\n",
    "print(summary)\n",
    "print('Outputs:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca6f166-1f22-4533-98cb-c2b16361a1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Este resumo esta muito melhor do que o ultimo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ccbc1e-bb41-46eb-b68c-a2ce11e0cdb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt Engennering with the FLAN-T5 template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2760838-a18d-40b8-bf55-bff08e220fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FLAN-T5- modelo has a lot of prompt enginnering templates [here](https://github.com/google-research/FLAN/tree/main/flan/v2) that can be pass to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c15b340a-e906-4313-b0f7-4060fc7b3ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Inputs:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "Outputs:\n",
      "The memo is going to go out as an intra-office memo to all employees by this afternoon.\n"
     ]
    }
   ],
   "source": [
    "# Agora vamos passar para o modelo uma instrução pelo prompt para verificar o seu retorno, apos ser recebido o prompt\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "# Prompt possibility 1:\n",
    "prompt = f\"Dialogue:\\n{phrase}\\nWhat was going on\"\n",
    "# Prompt possibility 2:\n",
    "# pronpt = f\"Here is a dialogue:\\n{phrase}\\n\\nWrite a short summary!\"\n",
    "\n",
    "# Vamos agora testar como o modelo desempenho uma tarefa de previsao sem prompt_enginnering.\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')    # Returning pythorch tensors\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('Example:')\n",
    "print(phrase)\n",
    "print('Summary:')\n",
    "print(summary)\n",
    "print('Outputs:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35bb9e73-2ce9-49e2-94f8-d63f4a0530f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt Engenering with one-shot and few-show inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd056a-6545-4846-96e4-2a195ae54d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f5ecab2-23d3-4d58-85eb-399e772e2f91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Inputs:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "Outputs:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n"
     ]
    }
   ],
   "source": [
    "# Agora vamos passar para o modelo uma instrução pelo prompt para verificar o seu retorno, apos ser recebido o prompt\n",
    "base_phrase = dataset['test'][1]['dialogue']\n",
    "base_summary = dataset['test'][1]['summary']\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{base_phrase}\n",
    "\n",
    "What was going on?\n",
    "{base_summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{phrase}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "# Vamos agora testar como o modelo desempenho uma tarefa de previsao sem prompt_enginnering.\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')    # Returning pythorch tensors\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('Example:')\n",
    "print(phrase)\n",
    "print('Summary:')\n",
    "print(summary)\n",
    "print('Outputs:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "694925e8-eb4a-440a-b2de-9849c43b5730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Inputs:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "Outputs:\n",
      "Ms. Dawson asks Ms. Dawson to take a dictation for her.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot inference\n",
    "\n",
    "# Agora vamos passar para o modelo uma instrução pelo prompt para verificar o seu retorno, apos ser recebido o prompt\n",
    "base_phrase = dataset['test'][1]['dialogue']\n",
    "base_summary = dataset['test'][1]['summary']\n",
    "base_phrase_1 = dataset['test'][2]['dialogue']\n",
    "base_summary_1 = dataset['test'][2]['summary']\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{base_phrase}\n",
    "\n",
    "What was going on?\n",
    "{base_summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{base_phrase_1}\n",
    "\n",
    "What was going on?\n",
    "{base_summary_1}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{phrase}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "# Vamos agora testar como o modelo desempenho uma tarefa de previsao sem prompt_enginnering.\n",
    "phrase = dataset['test'][0]['dialogue']\n",
    "summary = dataset['test'][0]['summary']\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')    # Returning pythorch tensors\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('Example:')\n",
    "print(phrase)\n",
    "print('Inputs:')\n",
    "print(summary)\n",
    "print('Outputs:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946f94a-903b-4924-bfdd-8ffa3563cd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
